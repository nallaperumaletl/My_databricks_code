{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "45ef1ea6-e233-4596-9b6a-6f94ae5d7b90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "># Sample SCD 1 code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63d68256-8d3c-4daa-a468-42eca8c6cd03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PARAMETERS:\n",
    "\n",
    "catalog = 'dev'\n",
    "\n",
    "# Schemas\n",
    "bronze_db = 'db1_bronze_raw'\n",
    "silver_db = 'db1_silver'\n",
    "gold_db = 'db1_gold'\n",
    "\n",
    "# Table\n",
    "trans_tbl = 'transactions'\n",
    "cleaned_tbl = 'clean_transactions'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e413851b-9c1e-41e2-85db-cb31547a0020",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    Row(id=1, name=\"Raj\", age=30),\n",
    "    Row(id=2, name=\"Virat\", age=25),\n",
    "    Row(id=3, name=\"Mahesh\", age=35)\n",
    "]\n",
    "\n",
    "# Create DataFrame and temp view\n",
    "df = spark.createDataFrame(data)\n",
    "df.createOrReplaceTempView(\"src\")\n",
    "\n",
    "# Show the temp view\n",
    "display(spark.sql(\"SELECT * FROM src\"))\n",
    "\n",
    "# Define target table and primary key columns\n",
    "target_table = \"default.sample_target\"\n",
    "pk_cols = [\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d08e35b7-f8bd-44a8-bd34-facfc3038b04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create target table if not exists\n",
    "create_table_query = f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {target_table}\n",
    "    USING DELTA AS SELECT * FROM src WHERE 1 = 0\n",
    "\"\"\"\n",
    "print(create_table_query)\n",
    "spark.sql(create_table_query)\n",
    "\n",
    "# Build ON expression for MERGE\n",
    "on_expr = \" AND \".join([f\"t.{c} = s.{c}\" for c in pk_cols])\n",
    "\n",
    "print(on_expr)\n",
    "\n",
    "# Perform MERGE (SCD-1 upsert)\n",
    "merge_query = f\"\"\"\n",
    "    MERGE INTO {target_table} t\n",
    "    USING src s\n",
    "    ON   {on_expr}\n",
    "    WHEN MATCHED THEN UPDATE SET *\n",
    "    WHEN NOT MATCHED THEN INSERT *\n",
    "\"\"\"\n",
    "print(merge_query)\n",
    "spark.sql(merge_query)\n",
    "\n",
    "# Show the target table after merge\n",
    "display(spark.table(target_table))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c1cc3bd-1409-4e7f-8a60-24501223148e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc65ba00-0440-4678-9137-237a27c694e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Actual Code :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac495176-5e33-4507-8f1e-62de623063fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook: silver_to_gold_star_schema.py\n",
    "# ==================================================\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import (\n",
    "    col, expr, sequence, to_date, explode, lit, date_format\n",
    ")\n",
    "\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 1. Minimal SCD-1 helper (Delta MERGE)\n",
    "# ---------------------------------------------------------------------------\n",
    "def upsert_scd1(src: DataFrame, target_table: str, pk_cols: list) -> None:\n",
    "    \"\"\"\n",
    "    Overwrite-on-key (SCD-1) into a Delta table.\n",
    "    If the target doesn't exist, it's created with the src schema.\n",
    "    \"\"\"\n",
    "    # Register source DataFrame as temp view for SQL operations\n",
    "    src.createOrReplaceTempView(\"src\")\n",
    "\n",
    "    # Create target Delta table if it does not exist\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {target_table}\n",
    "        USING DELTA AS SELECT * FROM src WHERE 1 = 0\n",
    "    \"\"\")\n",
    "\n",
    "    # Build ON clause for MERGE using primary key columns\n",
    "    on_expr = \" AND \".join([f\"t.{c} = s.{c}\" for c in pk_cols])\n",
    "\n",
    "    # Perform SCD-1 upsert using Delta MERGE\n",
    "    spark.sql(f\"\"\"\n",
    "        MERGE INTO {target_table} t\n",
    "        USING src s\n",
    "        ON   {on_expr}\n",
    "        WHEN MATCHED THEN UPDATE SET *\n",
    "        WHEN NOT MATCHED THEN INSERT *\n",
    "    \"\"\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 2. Build / refresh DATE dimension (dev.gold_db.dim_date)\n",
    "# ---------------------------------------------------------------------------\n",
    "def refresh_dim_date(start=\"2000-01-01\", end=\"2035-12-31\") -> None:\n",
    "    # Generate a date sequence DataFrame for the date dimension\n",
    "    df = (\n",
    "        spark.range(0, 1)\n",
    "        .select(sequence(to_date(lit(start)), to_date(lit(end))).alias(\"date_seq\"))\n",
    "        .select(explode(\"date_seq\").alias(\"Date_ID\"))\n",
    "        .withColumn(\"Year\", expr(\"year(Date_ID)\"))\n",
    "        .withColumn(\"Month\", expr(\"month(Date_ID)\"))\n",
    "        .withColumn(\"Day\", expr(\"day(Date_ID)\"))\n",
    "        .withColumn(\"Week\", expr(\"weekofyear(Date_ID)\"))\n",
    "        .withColumn(\"Quarter\", expr(\"(month(Date_ID)-1)/3 + 1\"))\n",
    "        .withColumn(\"Weekday\", date_format(col(\"Date_ID\"), \"EEEE\"))\n",
    "    )\n",
    "\n",
    "    # Overwrite the dim_date table with the new date dimension data\n",
    "    (df.write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"overwrite\")\n",
    "        .option(\"overwriteSchema\", \"true\")\n",
    "        .saveAsTable(f\"{catalog}.{gold_db}.dim_date\"))\n",
    "\n",
    "    # display(df)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 3. Distribute silver layer to gold star-schema\n",
    "# ---------------------------------------------------------------------------\n",
    "# The \"-> None\" in the function definition indicates that the function does not return any value.\n",
    "# It is a type hint specifying that the function's return type is None.\n",
    "def distribute_to_gold() -> None:\n",
    "    # Read cleaned transactions from silver layer\n",
    "    df = spark.table(f\"{catalog}.{silver_db}.{cleaned_tbl}\")\n",
    "\n",
    "    # ----- Dimensions -----\n",
    "    # Upsert customer dimension\n",
    "    upsert_scd1(\n",
    "        df.select(\n",
    "            \"Customer_ID\", \"Customer_Name\", \"Gender\", \"DOB\",\n",
    "            \"Email\", \"Phone\", \"Customer_City\"\n",
    "        ).distinct(),\n",
    "        f\"{catalog}.{gold_db}.dim_customer\",\n",
    "        [\"Customer_ID\"]\n",
    "    )\n",
    "\n",
    "    # Upsert merchant dimension\n",
    "    upsert_scd1(\n",
    "        df.select(\n",
    "            \"Merchant_ID\", \"Merchant_Name\",\n",
    "            \"Merchant_Category\", \"Merchant_Country\"\n",
    "        ).distinct(),\n",
    "        f\"{catalog}.{gold_db}.dim_merchant\",\n",
    "        [\"Merchant_ID\"]\n",
    "    )\n",
    "\n",
    "    # Upsert card dimension\n",
    "    upsert_scd1(\n",
    "        df.select(\n",
    "            \"Card_ID\", \"Card_Type\", \"Issuer_Bank\",\n",
    "            \"Card_Tier\", \"Expiry_Date\"\n",
    "        ).distinct(),\n",
    "        f\"{catalog}.{gold_db}.dim_card\",\n",
    "        [\"Card_ID\"]\n",
    "    )\n",
    "\n",
    "    # Upsert location dimension\n",
    "    upsert_scd1(\n",
    "        df.select(\n",
    "            \"Location_ID\", \"City\", \"State\", \"Country\"\n",
    "        ).distinct(),\n",
    "        f\"{catalog}.{gold_db}.dim_location\",\n",
    "        [\"Location_ID\"]\n",
    "    )\n",
    "\n",
    "    # ----- Fact -----\n",
    "    # Prepare fact table DataFrame with foreign keys to dimensions\n",
    "    fact_df = (\n",
    "        df.select(\n",
    "            \"Transaction_ID\",\n",
    "            \"Transaction_Date\",\n",
    "            \"Transaction_Amount\",\n",
    "            \"Transaction_Status\",\n",
    "            \"Transaction_Type\",\n",
    "            \"Customer_ID\",\n",
    "            \"Card_ID\",\n",
    "            \"Merchant_ID\",\n",
    "            \"Location_ID\",\n",
    "            col(\"Transaction_Date\").alias(\"Transaction_Date_ID\")  # FK -> dim_date\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Append new fact records to the fact_transactions table\n",
    "    (fact_df.write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"append\")\n",
    "        .saveAsTable(f\"{catalog}.{gold_db}.fact_transactions\"))\n",
    "\n",
    "    # display(fact_df)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 4. Orchestration entry point\n",
    "# ---------------------------------------------------------------------------\n",
    "def run_star_schema_refresh() -> str:\n",
    "    \"\"\"\n",
    "    Databricks Job entry:\n",
    "      1. Refresh dim_date\n",
    "      2. Push silver to gold star-schema\n",
    "    \"\"\"\n",
    "    # Refresh the date dimension table\n",
    "    refresh_dim_date()\n",
    "    # Distribute silver data to gold star-schema (dimensions and fact)\n",
    "    distribute_to_gold()\n",
    "    return \"Silver -> Gold star-schema refresh complete.\"\n",
    "\n",
    "# For ad-hoc runs in a notebook:\n",
    "if __name__ == \"__main__\":\n",
    "    print(run_star_schema_refresh())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22ff7fd0-7a3a-4899-9fce-a26a96f9878a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from dev.db1_gold.dim_customer;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd02b6af-3def-43c5-a09a-c455c7b54704",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from dev.db1_gold.fact_transactions limit 50;"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4543889537808269,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "03-silver_to_gold",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
